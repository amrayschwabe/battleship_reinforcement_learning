{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Reinforcement Learning -- Battleship\n",
    "##### JSL September 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-99381004021b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1.1 Import tensorflow and other libraries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# 1.1 Import tensorflow and other libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-429f2da88de6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0moutput_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBOARD_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0minput_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBOARD_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m          \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# 1.2 Define the nn variable network.\n",
    "# Input is array of BOARD_SIZE values.\n",
    "# ---------------------------------------\n",
    "#  -1 value -> Not yet checked\n",
    "#   0 value -> Checked, no ship\n",
    "#   1 value -> Checked, is ship location.\n",
    "# ---------------------------------------\n",
    "BOARD_SIZE = 10\n",
    "SHIP_SIZE = 3\n",
    "\n",
    "hidden_units = BOARD_SIZE\n",
    "output_units = BOARD_SIZE\n",
    "\n",
    "input_positions = tf.placeholder(tf.float32, shape=(1, BOARD_SIZE))\n",
    "labels =          tf.placeholder(tf.int64)\n",
    "learning_rate =   tf.placeholder(tf.float32, shape=[])\n",
    "# Generate hidden layer\n",
    "W1 = tf.Variable(tf.truncated_normal([BOARD_SIZE, hidden_units],\n",
    "             stddev=0.1 / np.sqrt(float(BOARD_SIZE))))\n",
    "b1 = tf.Variable(tf.zeros([1, hidden_units]))\n",
    "h1 = tf.tanh(tf.matmul(input_positions, W1) + b1)\n",
    "# Second layer -- linear classifier for action logits\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_units, output_units],\n",
    "             stddev=0.1 / np.sqrt(float(hidden_units))))\n",
    "b2 = tf.Variable(tf.zeros([1, output_units]))\n",
    "logits = tf.matmul(h1, W2) + b2 \n",
    "probabilities = tf.nn.softmax(logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.3 Define the operations we will use\n",
    "init = tf.initialize_all_variables()\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "train_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cross_entropy)\n",
    "# Start TF session\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.4 Game play definition.\n",
    "TRAINING = True\n",
    "def play_game(training=TRAINING):\n",
    "    \"\"\" Play game of battleship using network.\"\"\"\n",
    "    # Select random location for ship\n",
    "    ship_left = np.random.randint(BOARD_SIZE - SHIP_SIZE + 1)\n",
    "    ship_positions = set(range(ship_left, ship_left + SHIP_SIZE))\n",
    "    # Initialize logs for game\n",
    "    board_position_log = []\n",
    "    action_log = []\n",
    "    hit_log = []\n",
    "    # Play through game\n",
    "    current_board = [[-1 for i in range(BOARD_SIZE)]]\n",
    "    while (sum(hit_log) < SHIP_SIZE) and (len(action_log) < BOARD_SIZE):\n",
    "        board_position_log.append([[i for i in current_board[0]]])\n",
    "        probs = sess.run([probabilities], feed_dict={input_positions:current_board})[0][0]\n",
    "        probs = [p * (index not in action_log) for index, p in enumerate(probs)]\n",
    "        probs = [p / sum(probs) for p in probs]\n",
    "        if training == True:\n",
    "            bomb_index = np.random.choice(BOARD_SIZE, p=probs)            \n",
    "        else:\n",
    "            bomb_index = np.argmax(probs)\n",
    "        # update board, logs\n",
    "        hit_log.append(1 * (bomb_index in ship_positions))\n",
    "        current_board[0][bomb_index] = 1 * (bomb_index in ship_positions)\n",
    "        action_log.append(bomb_index)\n",
    "    return board_position_log, action_log, hit_log\n",
    "# Example:\n",
    "play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Reward function definition\n",
    "def rewards_calculator(hit_log, gamma=0.5):\n",
    "    \"\"\" Discounted sum of future hits over trajectory\"\"\"            \n",
    "    hit_log_weighted = [(item -  \n",
    "                         float(SHIP_SIZE - sum(hit_log[:index])) / float(BOARD_SIZE - index)) * (\n",
    "            gamma ** index) for index, item in enumerate(hit_log)]\n",
    "    return [((gamma) ** (-i)) * sum(hit_log_weighted[i:]) for i in range(len(hit_log))]\n",
    "\n",
    "# Example\n",
    "rewards_calculator([0,0,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.6 Training loop: Play and learn\n",
    "game_lengths = []\n",
    "TRAINING = True   # Boolean specifies training mode\n",
    "ALPHA = 0.06      # step size\n",
    "\n",
    "for game in range(10000):\n",
    "    board_position_log, action_log, hit_log = play_game(training=TRAINING)\n",
    "    game_lengths.append(len(action_log))\n",
    "    rewards_log = rewards_calculator(hit_log)\n",
    "    for reward, current_board, action in zip(rewards_log, board_position_log, action_log):\n",
    "        # Take step along gradient\n",
    "        if TRAINING:\n",
    "            sess.run([train_step], \n",
    "                feed_dict={input_positions:current_board, labels:[action], learning_rate:ALPHA * reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Plot running average game lengths\n",
    "window_size = 500\n",
    "running_average_length = [np.mean(game_lengths[i:i+window_size]) for i in range(len(game_lengths)- window_size)]\n",
    "pylab.plot(running_average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 Example showing how to print current coefficient values\n",
    "sess.run([W1], \n",
    "             feed_dict={input_positions:board_position_log[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
